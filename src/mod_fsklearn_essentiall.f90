!---------------------------------------------------------
! mod_fsklearn_essential
!---------------------------------------------------------
!
!   - In this module, some essential parts for the fsklearn
!     are defined. They include three types of essential
!     functions, some files and some derived type variables.
!  - They will be used in mod_fsklearn to build the full
!     machine learning module.
!
!--------------------------------------------------------
! Interfaces:
!--------------------------------------------------------
!
!   - Include the following components:
!   - 1) Read_XX:
!     + Read_Neural_Network
!     + Read_Decision_Tree
!     + Read_Random_Forest
!   - 2) Predict_XX:
!     + Predict_Neural_Network
!     + Predict_Decision_Tree
!     + Predict_Random_Forest
!   - 3) Activation_XX:
!     + Activation_logistic
!     + Activation_tanh
!     + Activation_ReLU
!     + Activation_identity
!     + Activation_softmax
!  - For more details, see the corresponding region for
!    each function.
!
!--------------------------------------------------------
! Files:
!--------------------------------------------------------
!
! - Files associate with Fsklearn. If not set, the path
!   of the training data and the training coefficients
!   will be "fsklearn_data" and "fsklearn_coef" respectively
!
! - By dedault, only the "dsklearn_data", "fsklearn_coef"
!   needed to be set in fsklearn_initialization.
!
! - "coef_files_path":
!   + the path for fsklearn related coefficients
!
! - "set_ML_file":
!   + File name for setting coefficients
!   + Located in the path "coef_files_path"
!   + need to assign input vector length, output vector
!     length, training coefficients and some other
!     custom coefficients
!
! - "f2py_training_coef":
!   + json file that passes the coefficient to Python
!   + Located in the path "coef_files_path"
!
! - "nn_coef.dat":
!   + Neural network coefficients from Python code
!   + Located in the path "coef_files_path"
!
! - "dt_coefficients.dat":
!   + Decision tree coefficients from Python code
!   + Located in the path "coef_files_path"
!
! - "rf_coef.dat":
!   + Random forest coefficients from Python code
!   + Located in the path "coef_files_path"
!
! - "training_data_path":
!   + the path for training data
!
! - "training_py":
!   + python code generated by Fortran code
!
! - "training_data_path":
!   + the path for training data
! -"training_input_name":
!   + File name for input data
!   + Located in the path "training_data_path"
!   + The length of the data in each row is n_inputs
!   + If mpi with N processot is defined, there will be
!     N input files, Then all files will be gathered
!     automatically by the Python code.
!
! - "training_output_name":
!   + Located in the path "training_data_path"
!   + file name for put put data
!   + The length of the data in each row is n_outputs
!     N input files, Then all files will be gathered
!     automatically by the Python code.
!
!--------------------------------------------------------
! Module variables:
!--------------------------------------------------------
!
! PS:
!    - Precision for float number.
!    - PS = 4: single precision
!    - PS = 8: double precision
!
! Fsklearn_Define:
!    - Derived type that contains the machine learning
!       variables. It can do training and prediction
!       with a uniform way.
!    - %n_inputs:
!       + length of the input vector
!       + It is necessary to provide the length of the
!         input vector during initialization.
!    - %n_outputs:
!       + length of the output vector
!       + It is necessary to provide the length of the
!         output vector during initialization.
!    - %Inputs:
!       + Input vector, not necessary
!    - %Outputs:
!       + Output vector, not necessary
!    - %Coef_Read:
!       + Read the coefficients and coefficients during
!         initialization for PREDICTION.
!    - %Predict:
!       + Predict procedure
!    - %Gen_Training:
!       + Procedure for generate training data, can be
!         point to other subroutines.
!    - %Predict:
!       + Procedure for initialization, can be point to
!         other subroutines.
!
! Ragged_Vector:
!    - Declaration of ragged vector and ragged matrix.
!    - In this case, designed for neural networks.
!    - Ragged vectors for 2-D array consists of vectors with
!      different length
!    - Ragged vectors for 3-D array consists of matrice with
!      different sized
!
! NN_activation:
!    - Procedure used to choose activation function
!      at the beginning.
!
! Neural_Network:
!    - Derived Type containing all Neural network variables.
!    - %input_len:
!       + length of the input vector
!    - %output_len:
!       + length of the output vector
!    - %layers:
!       + layer of neural network
!    - %layer_size:
!       + 1-D vector with $(%layers) length, each element
!         each element represents the represents the number
!         of the neural in the corresponding layer.
!    - %Activations:
!       + Activation functions for each layer
!       + Ragged vector with $(%layers)-1 vectors, the length
!         of each vector Activations(i)%Vec is the same as
!         $(layer_size(i))
!    - %Intercepts:
!       + bias variable for each layer
!       + Ragged vector with $(%layers)-1 vectors, the length
!         of each vector Intercepts(i)%Vec is the same as
!         $(layer_size(i))
!    - %Coefs:
!       + coefficient for each neural
!       + Ragged matrix with $(%layers)-1 matrices
!       + The size Coefs(i)%Mat is
!         [$(layer_size(i+1)),$(layer_size(i))]
!    - %Activation type:
!       + Activation function. Contains 5 types:
!        'sigmoid', 'tanh', 'ReLU', 'ideneity', 'softmax'
!    - %Out_activation type:
!       + Activation function for output. Contains 5 types:
!        'sigmoid', 'tanh', 'ReLU', 'ideneity', 'softmax'
!    - %Activation:
!       + Uniform procedure calling activation function
!    - %Out_activation:
!       + Uniform procedure calling out activation function
!    - %Coef_Read:
!       + Uniform procedure reading Neural Network Coefficients
!    - %Predict:
!       + Uniform procedure predicting Neural Network results
!
! Nodes:
!    - Derived type for binary tree node class
!    - Used in decision tree and random forest method
!    - %children_left:
!       + Node number of the left child
!    - %children_right:
!       + Node number of the Right child
!    - %feature:
!       + Choice of the input feature for logistic operation
!    - %threshold:
!       + Threshold for logistic operation
!    - %Value:
!       + Value of the predicted result for the corresponding
!         node. with the length of $(n_outputs)
!
! Decision_Tree:
!    - Derived Type containing all Decision Tree variables.
!    - %node_count
!       + amount of the nodes
!    - %n_inputs
!       + length of the input variables
!    - %n_outputs
!       + length of the output variables
!    - %max_depth
!       + maximum depth of the tree
!    - %Node
!       + nodes in decision trees, Nodes type
!
! Random_Forest:
!    - Derived Type containing all Random_Forest variables.
!    - %tree_count
!       + amount of the decision treees
!    - %n_inputs
!       + length of the input variables
!    - %n_outputs
!       + length of the output variables
!    - %Trees
!       + trees in random forest, Decision_Tree type
!
! by Zhouteng Ye
! Last update: 04/17/2019
!
!---------------------------------------------------------

Module Mod_Fsklearn_Essential

  ! Subject to the choice of precision.
  ! Single precision by default
# if defined(DOUBLE_PRECISION)
  Integer, Private, Parameter :: PS = 8
# else
  Integer, Private, Parameter :: PS = 4
# endif

  Type Ragged_Vector
    Real(PS), Allocatable :: Vec(:)
  End Type Ragged_vector
  Type Ragged_Matrix
    Real(PS), Allocatable :: Mat(:,:)
  End Type Ragged_Matrix

  !↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓Neural Networks variables↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓
  Type :: NN_activation
    Procedure(Sub_Interface), Pointer, NoPass :: activate => NULL()
  End Type NN_activation

  ! interface for choose activation type
  Interface
    Function Sub_Interface(n, X)
      Import :: PS
      Integer,  Intent(in) :: n
      Real(PS), Intent(in), Dimension(n) :: X
      Real(PS), Dimension(n) :: Sub_Interface
    End Function Sub_Interface
  End Interface

  ! neural network coefficients
  Type :: Neural_Network
    Integer :: input_len
    Integer :: output_len
    Integer :: layers
    Integer, Allocatable :: Layer_Size(:)
    Type(Ragged_Vector), Allocatable :: Activations(:)
    Type(Ragged_Vector), Allocatable :: Intercepts(:)
    Type(Ragged_Matrix), Allocatable :: Coefs(:)
    Character(10) :: Activation_type
    Character(10) :: out_Activation_type
    Type(NN_Activation) :: Activation
    Type(NN_Activation) :: Out_Activation
  Contains
    Procedure , NoPass :: Coef_Read => Read_Neural_Network
    Procedure , NoPass :: Predict   => Predict_Neural_Network
  End Type Neural_Network
  !↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑End Neural Network Variables↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑


  !↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓Decision Tree Variables↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓
  Type Nodes
    Integer  :: children_left
    Integer  :: children_right
    Integer  :: feature
    Real(PS) :: threshold
    Real(PS), Allocatable :: Values(:)
  End Type Nodes

  Type:: Decision_Tree
    Integer :: node_count
    Integer :: n_inputs
    Integer :: n_outputs
    Integer :: max_depth
    Type(Nodes), Allocatable :: Node(:)
  Contains
    Procedure , NoPass :: Coef_Read => Read_Decision_Tree
    Procedure , NoPass :: Predict => Predict_Decision_Tree
  End Type Decision_Tree
  !↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑End Decision Tree Variables↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑


  !↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓Random Forest Variables↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓
  Type:: Random_Forest
    Integer :: tree_count
    Integer :: n_inputs
    Integer :: n_outputs
    Type(Decision_Tree), Allocatable :: Trees(:)
  Contains
    Procedure , NoPass :: Coef_Read => Read_Random_Forest
    Procedure , NoPass :: Predict => Predict_Random_Forest
  End Type Random_Forest
  !↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑End Random Forest Variables↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑

  !↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓Python_File_Generator↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓
  Type :: NN_Parameters
    Integer :: num_para = 21
    Character(50) :: key(21)
    Character(50) :: value(21)
  Contains
    Procedure :: NN_Load_Default_Para
    Procedure :: NN_Read_and_Update_Para
  End Type NN_Parameters

  Type :: DT_Parameters
    Integer :: num_para = 12
    Character(50) :: key(12)
    Character(50) :: value(12)
  Contains
    Procedure :: DT_Load_Default_Para
    Procedure :: DT_Read_and_Update_Para
  End Type DT_Parameters

  Type :: RF_Parameters
    Integer :: num_para = 16
    Character(50) :: key(16)
    Character(50) :: value(16)
  Contains
    Procedure :: RF_Load_Default_Para
    Procedure :: RF_Read_and_Update_Para
  End Type RF_Parameters

  Type :: Generate_Python
    Character(20) :: training_type
    Character(1000) :: Training_script
    Type(NN_Parameters) :: NN_Para
    Type(DT_Parameters) :: DT_Para
    Type(RF_Parameters) :: RF_Para
  Contains
    Procedure :: Py_Import
    Procedure :: Generate_Training_PY
    Procedure :: PY_data_processing
    Procedure :: sk2f
    Procedure :: Gen_Training_Param
  End Type Generate_Python

  !↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑End Python_file_generator↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑
  
  !↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓Files↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓
  Character(100) :: Coef_files_path
  Character(100) :: set_ML_file         = 'fsklearn_coef.namelist'
  Character(100) :: f2py_training_coef  = 'training_coef.json'
  Character(100) :: nn_coef_name        = 'nn_coef.dat'
  Character(100) :: dt_coef_name       = 'dt_coef.dat'
  Character(100) :: rf_coef_name       = 'rf_coef.dat'
  Character(100) :: training_py         = 'training.py'

  Character(100) :: training_data_path
  Character(100) :: training_input_name   = 'training_input'
  Character(100) :: training_output_name  = 'training_output'
  !↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑End Files↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑

  ! Predefined type variables
  Type(Neural_Network) :: N_Work
  Type(Decision_Tree) :: D_Tree
  Type(Random_Forest) :: R_Forest

Contains

  Subroutine Read_Neural_Network

# if defined (PARALLEL)
    Use mpi
# endif
    Implicit None

    Integer :: i,j
    Integer :: error
    Character(100) :: string

    Character :: activation
    Character :: out_activation
    character(100) :: tmp, tmp1
    Type(Decision_Tree) :: tree1
    Integer :: n_proc
    Integer :: myid
    Character(10) :: string_myid
    Integer :: ier

    tmp = Trim(adjustl(coef_files_path))// &
        Trim(adjustl(nn_coef_name))

# if defined (PARALLEL)
! Parallel version
    ! ugly mpi implementation here
    Call MPI_COMM_RANK(MPI_COMM_WORLD, myid, ier)
    Call MPI_COMM_SIZE(MPI_COMM_WORLD, n_proc, ier)
    Write(string_myid,"(I10)") myid
    tmp1 = Trim(Adjustl(tmp))// &
        Trim(Adjustl(string_myid))
    Do i = 0, n_proc-1
      If (myid .eq. i) Then
        Call Execute_Command_Line('cp '//tmp//' '//tmp1)
      End If
    End Do

    open(4000+myid, file=tmp,status='unknown')

    Read(4000+myid, *,iostat=error) string
    Read(4000+myid, *) N_Work%layers
    Allocate(N_Work%layer_size(N_Work%layers))

    Read(4000+myid, *,iostat=error) string
    Read(4000+myid, *) N_Work%layer_size
    N_Work%input_len = N_Work%layer_size(1)
    N_Work%output_len = N_Work%layer_size(N_Work%layers)

    Allocate(N_Work%activations(N_Work%layers))
    Do i = 1,N_Work%layers
      Allocate(N_Work%activations(i)%vec(N_Work%layer_size(i)))
    End do

    Read(4000+myid, *,iostat=error) string
    Allocate(N_Work%intercepts(N_Work%layers-1))
    Do i = 1,N_Work%layers-1
      Allocate(N_Work%intercepts(i)%vec(N_Work%layer_size(i+1)))
      Read(4000+myid, *) N_Work%intercepts(i)%vec
    End do

    Read(4000+myid, *,iostat=error) string
    Allocate(N_Work%coefs(N_Work%layers-1))
    Do i = 1,N_Work%layers-1
      Allocate(N_Work%coefs(i)%mat(N_Work%layer_size(i+1),N_Work%layer_size(i)))
      Read(4000+myid, *,iostat=error) string
      Do j = 1,N_Work%layer_size(i+1)
        Read(4000+myid, *) N_Work%coefs(i)%mat(j,:)
      End do
    End do

    Read(4000+myid, *,iostat=error) string
    Read(4000+myid, *) N_Work%Activation_type
    Read(4000+myid, *,iostat=error) string
    Read(4000+myid, *) N_Work%out_Activation_type

    Close(81)

    if (Trim(N_Work%Activation_type).eq.'logistic') Then
      N_Work%activation%activate => Activation_logistic
    else if (Trim(N_Work%Activation_type).eq.'tanh') Then
      N_Work%activation%activate => Activation_tanh
    else if (Trim(N_Work%Activation_type).eq.'softmax') Then
      N_Work%activation%activate => Activation_softmax
    else if (Trim(N_Work%Activation_type).eq.'relu') Then
      N_Work%activation%activate => Activation_ReLU
    else if (Trim(N_Work%Activation_type).eq.'identity') Then
      N_Work%activation%activate => Activation_identity
    else
      write(*,*) 'invalid activation type'
      Stop
    End if

    If (Trim(N_Work%out_Activation_type).eq.'logistic') Then
      N_Work%out_activation%activate => Activation_logistic
    Else If (Trim(N_Work%out_Activation_type).eq.'tanh') Then
      N_Work%out_activation%activate => Activation_tanh
    Else If (Trim(N_Work%out_Activation_type).eq.'softmax') Then
      N_Work%out_activation%activate => Activation_softmax
    Else If (Trim(N_Work%out_Activation_type).eq.'relu') Then
      N_Work%out_activation%activate => Activation_ReLU
    Else If (Trim(N_Work%out_Activation_type).eq.'identity') Then
      N_Work%out_activation%activate => Activation_identity
    Else
      Write(*,*) 'invalid output activation type'
      Stop
    End if
# else
! sequential code

    open(81,file=tmp,status='unknown')

    Read(81,*,iostat=error) string
    Read(81,*) N_Work%layers
    Allocate(N_Work%layer_size(N_Work%layers))

    Read(81,*,iostat=error) string
    Read(81,*) N_Work%layer_size
    N_Work%input_len = N_Work%layer_size(1)
    N_Work%output_len = N_Work%layer_size(N_Work%layers)


    Allocate(N_Work%activations(N_Work%layers))
    Do i = 1,N_Work%layers
      Allocate(N_Work%activations(i)%vec(N_Work%layer_size(i)))
    End do

    Read(81,*,iostat=error) string
    Allocate(N_Work%intercepts(N_Work%layers-1))
    Do i = 1,N_Work%layers-1
      Allocate(N_Work%intercepts(i)%vec(N_Work%layer_size(i+1)))
      Read(81,*) N_Work%intercepts(i)%vec
    End do


    Read(81,*,iostat=error) string
    Allocate(N_Work%coefs(N_Work%layers-1))
    Do i = 1,N_Work%layers-1
      Allocate(N_Work%coefs(i)%mat(N_Work%layer_size(i+1),N_Work%layer_size(i)))
      Read(81,*,iostat=error) string
      Do j = 1,N_Work%layer_size(i+1)
        Read(81,*) N_Work%coefs(i)%mat(j,:)
      End do
    End do

    Read(81,*,iostat=error) string
    Read(81,*) N_Work%Activation_type
    Read(81,*,iostat=error) string
    Read(81,*) N_Work%out_Activation_type

    Close(81)

    If (Trim(N_Work%Activation_type).eq.'logistic') Then
      N_Work%activation%Activate => Activation_logistic
    Else If (Trim(N_Work%Activation_type).eq.'tanh') Then
      N_Work%activation%Activate => Activation_tanh
    Else If (Trim(N_Work%Activation_type).eq.'softmax') Then
      N_Work%activation%Activate => Activation_softmax
    Else If (Trim(N_Work%Activation_type).eq.'relu') Then
      N_Work%activation%Activate => Activation_ReLU
    Else If (Trim(N_Work%Activation_type).eq.'identity') Then
      N_Work%activation%Activate => Activation_identity
    Else
      Write(*,*) 'invalid activation type'
    End If

    If (Trim(N_Work%out_Activation_type).eq.'logistic') Then
      N_Work%out_activation%Activate => Activation_logistic
    Else If (Trim(N_Work%out_Activation_type).eq.'tanh') Then
      N_Work%out_activation%Activate => Activation_tanh
    Else If (Trim(N_Work%out_Activation_type).eq.'softmax') Then
      N_Work%out_activation%Activate => Activation_softmax
    Else If (Trim(N_Work%out_Activation_type).eq.'relu') Then
      N_Work%out_activation%Activate => Activation_ReLU
    Else If (Trim(N_Work%out_Activation_type).eq.'identity') Then
      N_Work%out_activation%Activate => Activation_identity
    Else
      Write(*,*) 'invalid output activation type'
    End If

# endif

  End Subroutine Read_Neural_Network

  Subroutine Read_Decision_Tree
    
# if defined (PARALLEL)
    Use mpi
# endif
    Implicit None
    Integer :: i,j
    Integer :: error
    Character(20) :: string
    Character(100) :: tmp, tmp1
    Integer :: n_proc
    Integer :: myid
    Character(10) :: string_myid
    Integer :: ier

    tmp = Trim(Adjustl(coef_files_path))// &
        Trim(Adjustl(dt_coef_name)) 

# if defined (PARALLEL)
! Parallel version
    ! ugly mpi implementation here
    Call MPI_COMM_RANK(MPI_COMM_WORLD, myid, ier)
    Call MPI_COMM_SIZE(MPI_COMM_WORLD, n_proc, ier)
    Write(string_myid,"(I10)") myid
    tmp1 = Trim(Adjustl(tmp))// &
        Trim(Adjustl(string_myid))
    Do i = 0, n_proc-1
      If (myid .eq. i) Then
        Call Execute_Command_Line('cp '//tmp//' '//tmp1)
      End If
    End Do

    Open(4000+myid, file=tmp, status='unknown')

    ! tree related coefficients
    Read(4000+myid,*,iostat=error) string
    Read(4000+myid,*) D_Tree%node_count
    Read(4000+myid,*,iostat=error) string
    Read(4000+myid,*) D_Tree%n_inputs
    Read(4000+myid,*,iostat=error) string
    Read(4000+myid,*) D_Tree%n_outputs
    Read(4000+myid,*,iostat=error) string
    Read(4000+myid,*) D_Tree%max_depth

    Allocate(D_Tree%node(D_TREE%node_count))

    ! binary tree
    Do i = 1,D_Tree%node_count
      Allocate(D_Tree%node(i)%values(D_TREE%n_outputs))
      Read(4000+myid,*,iostat=error) string
      Read(4000+myid,*) D_Tree%node(i)%children_left
      Read(4000+myid,*) D_Tree%node(i)%children_right
      Read(4000+myid,*) D_Tree%node(i)%feature
      Read(4000+myid,*) D_Tree%node(i)%threshold
      Read(4000+myid,*) D_Tree%node(i)%values
    End do

    Close(4000+myid)

    Do i = 0, n_proc-1
      If (myid .eq. i) Then
        Call Execute_Command_Line('rm '//tmp1)
      End If
    End Do

# else
! sequential version
    Open(82, file=tmp, status='unknown')

    Read(82,*,iostat=error) string
    Read(82,*) D_Tree%node_count
    Read(82,*,iostat=error) string
    Read(82,*) D_Tree%n_inputs
    Read(82,*,iostat=error) string
    Read(82,*) D_Tree%n_outputs
    Read(82,*,iostat=error) string
    Read(82,*) D_Tree%max_depth

    Allocate(D_Tree%node(D_TREE%node_count))

    Do i = 1,D_Tree%node_count
      Allocate(D_Tree%node(i)%values(D_TREE%n_outputs))
      Read(82,*,iostat=error) string
      Read(82,*) D_Tree%node(i)%children_left
      Read(82,*) D_Tree%node(i)%children_right
      Read(82,*) D_Tree%node(i)%feature
      Read(82,*) D_Tree%node(i)%threshold
      Read(82,*) D_Tree%node(i)%values
    End do

    Close(82)
# endif

  End Subroutine Read_Decision_Tree

  Subroutine Read_Random_Forest
# if defined(PARALLEL)
    Use mpi
#endif
    Implicit None
    Integer :: i,j
    Integer :: error
    Character(100) :: string
    character(100) :: tmp, tmp1
    Type(Decision_Tree) :: tree1
    Integer :: n_proc
    Integer :: myid
    Character(10) :: string_myid
    Integer :: ier

    tmp = Trim(Adjustl(coef_files_path))// &
        Trim(Adjustl(rf_coef_name)) 

# if defined (PARALLEL)
! Parallel version
    ! ugly mpi implementation here
    Call MPI_COMM_RANK(MPI_COMM_WORLD, myid, ier)
    Call MPI_COMM_SIZE(MPI_COMM_WORLD, n_proc, ier)
    Write(string_myid,"(I10)") myid
    tmp1 = Trim(Adjustl(tmp))// &
        Trim(Adjustl(string_myid))
    Do i = 0, n_proc-1
      If (myid .eq. i) Then
        Call Execute_Command_Line('cp '//tmp//' '//tmp1)
      End If
    End Do

    Open(4000+myid, file=tmp, status='unknown')
    Read(4000+myid, *, iostat=error) string
    Read(4000+myid, *) R_Forest%tree_count

    Allocate(R_Forest%trees(R_Forest%tree_count))

    ! Read from each decision trees
    Do j = 1, R_Forest%tree_count
      Read(4000+myid,*, iostat=error) string
      Read(4000+myid,*) R_Forest%trees(j)%node_count

      Read(4000+myid,*, iostat=error) string
      Read(4000+myid,*) R_Forest%trees(j)%n_inputs

      Read(4000+myid,*, iostat=error) string
      Read(4000+myid,*) R_Forest%trees(j)%n_outputs

      Read(4000+myid,*, iostat=error) string
      Read(4000+myid,*) R_Forest%trees(j)%max_depth

      Allocate(R_Forest%trees(j)%node(R_Forest%trees(j)%node_count))

      Do i = 1, R_Forest%trees(j)%node_count
        Allocate(R_Forest%trees(j)%node(i)%values(R_Forest%trees(j)%n_outputs))
        Read(4000+myid, *,iostat=error) string
        Read(4000+myid, *) R_Forest%trees(j)%node(i)%children_left
        Read(4000+myid, *) R_Forest%trees(j)%node(i)%children_right
        Read(4000+myid, *) R_Forest%trees(j)%node(i)%feature
        Read(4000+myid, *) R_Forest%trees(j)%node(i)%threshold
        Read(4000+myid, *) R_Forest%trees(j)%node(i)%values
      End Do
    End Do

    R_Forest%n_inputs  = R_Forest%trees(1)%n_inputs
    R_Forest%n_outputs = R_Forest%trees(1)%n_outputs

    Close(4000+myid)

    Do i = 0, n_proc-1
      If (myid .eq. i) Then
        Call Execute_Command_Line('rm '//tmp1)
      End If
    End Do

# else
!sequential version
    open(83,file=tmp,status='unknown')
    Read(83, *,iostat=error) string
    Read(83, *) R_Forest%tree_count

    Allocate(R_Forest%trees(R_Forest%tree_count))

    Do j = 1, R_Forest%tree_count
      Read(83,*,iostat=error) string
      Read(83,*) R_Forest%trees(j)%node_count
      Read(83,*,iostat=error) string
      Read(83,*) R_Forest%trees(j)%n_inputs
      Read(83,*,iostat=error) string
      Read(83,*) R_Forest%trees(j)%n_outputs
      Read(83,*,iostat=error) string
      Read(83,*) R_Forest%trees(j)%max_depth

      Allocate(R_Forest%trees(j)%node(R_Forest%trees(j)%node_count))

      Do i = 1,R_Forest%trees(j)%node_count
        Allocate(R_Forest%trees(j)%node(i)%values(R_Forest%trees(j)%n_outputs))
        Read(83,*,iostat=error) string
        Read(83,*) R_Forest%trees(j)%node(i)%children_left
        Read(83,*) R_Forest%trees(j)%node(i)%children_right
        Read(83,*) R_Forest%trees(j)%node(i)%feature
        Read(83,*) R_Forest%trees(j)%node(i)%threshold
        Read(83,*) R_Forest%trees(j)%node(i)%values
      End do
    End do

    R_Forest%n_inputs  = R_Forest%trees(1)%n_inputs
    R_Forest%n_outputs = R_Forest%trees(1)%n_outputs

    Close(83)
# endif

  End Subroutine read_Random_Forest

  !↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Predict Subroutines↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓
  function predict_Neural_Network(input, n_input, n_output)
    Implicit None
    integer :: n_input
    integer :: n_output
    Real(PS) :: input(n_input)
    Real(PS) :: predict_Neural_Network(n_output) 
    integer :: i

    N_Work%activations(1)%vec = input

    Do i = 1, N_Work%layers-2
      N_Work%activations(i+1)%vec = matmul(N_Work%coefs(i)%mat,N_Work%activations(i)%vec) + N_Work%intercepts(i)%vec
      N_Work%activations(i+1)%vec =N_Work%activation%activate(N_Work%layer_size(i+1),N_Work%activations(i+1)%vec)
    End do
    N_Work%activations(N_Work%layers)%vec = &
        matmul(N_Work%coefs(N_Work%layers-1)%mat,N_Work%activations(N_Work%layers-1)%vec) + N_Work%intercepts(N_Work%layers-1)%vec
    N_Work%activations(N_Work%layers)%vec = &
    N_Work%out_activation%activate(N_Work%output_len,N_Work%activations(N_Work%layers)%vec)

    predict_Neural_Network = N_Work%activations(N_Work%layers)%vec

  End function predict_Neural_Network

  function predict_Decision_Tree(input,n_input,n_output)
    Implicit None
    integer :: n_input
    integer :: n_output
    Real(PS) :: input(n_input)
    Real(PS) :: predict_Decision_Tree(n_output)

    integer :: i,n

    n = 1
    Do i = 1, D_Tree%max_depth
      if (D_Tree%node(n)%feature .eq. -1) Exit
      if (input(D_Tree%node(n)%feature) .le. D_TREE%node(n)%threshold) Then
        n = D_Tree%node(n)%children_left
      else
        n = D_Tree%node(n)%children_right
      End if
    End do

    predict_Decision_Tree = D_Tree%node(n)%values

  End function predict_Decision_Tree

  function predict_Random_Forest(input,n_input,n_output)
    Implicit None
    integer :: n_input
    integer :: n_output
    Real(PS) :: input(n_input)
    Real(PS) :: predict_Random_Forest(n_output)

    integer :: i, j, n

    predict_Random_Forest = 0.0_PS
    Do j = 1, R_Forest%tree_count
      n=1
      Do i = 1, R_Forest%trees(j)%max_depth
        if (R_Forest%trees(j)%node(n)%feature .eq. -1) Exit
        if (input(R_Forest%trees(j)%node(n)%feature) .le. R_Forest%trees(j)%node(n)%threshold) Then
          n = R_Forest%trees(j)%node(n)%children_left
        else
          n = R_Forest%trees(j)%node(n)%children_right
        End if
      End do
      predict_Random_Forest = predict_Random_Forest + R_Forest%trees(j)%node(n)%values
    End do

    predict_Random_Forest = predict_Random_Forest / R_Forest%tree_count

  End function predict_Random_Forest
  !↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑End Prediction Subroutines↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑


  !↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Generate_Training_Data↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓
  !-----------------------------------------------------
  ! In this part, the activation functions are defined.
  !   all the activation functions is called by a
  !   uniform procedure "Activation".
  !
  !-----------------------------------------------------
  ! I/O
  !-----------------------------------------------------
  !
  ! inputs:
  !   X:
  !     input vector, should be 1-D
  !   n:
  !     length of the input vector
  ! output:
  !    function result:
  !     output vector, should be 1-D with n length
  !
  !-----------------------------------------------------
  ! Activation functions
  !-----------------------------------------------------
  !
  ! Logistic function:
  !    f(x) = 1/(1+e^-x)
  !
  ! Tanh function:
  !    f(x) = tanh(x)
  !
  ! ReLU function:
  !    f(x) = max(0,x)
  !
  ! Identity function:
  !    f(x) = x
  !
  ! Softmax function:
  !    f(x)_i = exp(x_i)/sigma(x_i)
  !    Only used for classification.
  !    (Which means useless currently)
  !-----------------------------------------------------
  function Activation_logistic(n,X)
    Implicit None
    integer, Intent(in) :: n
    Real(PS), Intent(in), dimension(n) :: X
    Real(PS), dimension(n) :: Activation_logistic
    Activation_logistic = 1.0 / (1.0+exp(-X))
  End function Activation_logistic

  function Activation_tanh(n,X)
    Implicit None
    integer, Intent(in) :: n
    Real(PS), Intent(in), dimension(n) :: X
    Real(PS), dimension(n) :: Activation_tanh
    Activation_tanh = tanh(X)
  End function Activation_tanh

  function Activation_ReLU(n,X)
    Implicit None
    integer, Intent(in) :: n
    Real(PS), Intent(in), dimension(n) :: X
    Real(PS), dimension(n) :: Activation_ReLU
      Activation_ReLU = max(X,0.d0)
    End function Activation_ReLU

  function Activation_identity(n,X)
    Implicit None
    integer, Intent(in) :: n
    Real(PS), Intent(in), dimension(n) :: X
    Real(PS), dimension(n) :: Activation_identity
    Activation_identity = X
  End function Activation_identity

  function Activation_softmax(n,X)
    Implicit None
    Integer, Intent(in) :: n
    Real(PS), Intent(in), dimension(n) :: X
    Real(PS), dimension(n) :: tmp
    Real(PS), dimension(n) :: Activation_softmax
    tmp = exp(X - maxval(X))/sum(tmp)
    Activation_softmax = 1.0 / (1.0+exp(-X))
  End function Activation_softmax
  !↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑End Activation function↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑

  !↓↓↓↓↓↓↓↓↓↓ Generate_Training_Python file↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓
  !
  !  Very ugly subroutines. Simply write to .py file
  !  line by line. Will come back for further development
  !
  Subroutine Generate_Training_PY(self, file_num)
    Implicit None
    Class(Generate_Python) :: self
    Integer :: file_num

    Call self%PY_import(file_num)
    Call self%SK2F(file_num)
    Call self%PY_data_processing(file_num)

  End Subroutine Generate_Training_PY

  Subroutine PY_data_processing(self, file_num)
#if defined (Parallel)
    Use MPI
#endif
    Implicit None
    Class(Generate_Python) :: self
    Integer :: file_num
    Integer :: num_proc, ier

    write(file_num,'(A)') "T_data_path = '"//Trim(training_data_path) // "'"
    write(file_num,'(A)') "coef_path = '"//Trim(coef_files_path)//"'"
    write(file_num,'(A)') "training_type = '"//Trim(self%Training_type)//"'"
    write(file_num,'(A)') "input_name = '"//Trim(training_input_name)//"'"
    write(file_num,'(A)') "output_name = '"//Trim(training_output_name)//"'"
#if defined (Parallel)
    write(file_num,'(A)') "T_input  = np.loadtxt(T_data_path + input_name+'0.dat')"
    write(file_num,'(A)') "T_output  = np.loadtxt(T_data_path + output_name+'0.dat')"
    Call MPI_Comm_size( MPI_COMM_WORLD, num_proc, ier )
    write(file_num,'(A)') "for i in range("//num2str(num_proc-1)//"):"
    write(file_num,'(A)') "    file_name = T_data_path+ input_name + str(i+1) + '.dat'"
    write(file_num,'(A)') "    file_name1 = T_data_path + output_name + str(i+1) + '.dat'"
    write(file_num,'(A)') "    T_input = np.append(T_input,np.loadtxt(file_name), axis=0)"
    write(file_num,'(A)') "    T_output = np.append(T_output,np.loadtxt(file_name1), axis=0)"
#else
    write(file_num,'(A)') "T_input  = np.loadtxt(T_data_path + input_name+ '.dat')"
    write(file_num,'(A)') "T_output  = np.loadtxt(T_data_path + output_name + '.dat')"
#endif

    write(file_num,'(A)') self%Training_script

    write(file_num,'(A)') "ml.fit(T_input, T_output)"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "ml.type = training_type"
    write(file_num,'(A)') "ml.output_coef_path = coef_path"
    write(file_num,'(A)') "sk2f(ml)"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "print('-------------------------------\n')"
    write(file_num,'(A)') "print('training_completed\n\n')"
    write(file_num,'(A)') "print('training_type:\n')"
    write(file_num,'(A)') "print(training_type+'\n')"
    write(file_num,'(A)') "print('-------------------------------\n')"

  End Subroutine PY_data_processing

  Subroutine Py_import(self, file_num)
    Implicit None
    Class(Generate_Python) :: self
    Integer :: file_num
    write(file_num,'(A)') "import json"
    write(file_num,'(A)') "import numpy as np"
    write(file_num,'(A)') "from sklearn.neural_network import MLPRegressor"
    write(file_num,'(A)') "from sklearn.ensemble import RandomForestRegressor"
    write(file_num,'(A)') "from sklearn import tree"
    write(file_num,'(A)') "from sklearn.multioutput import MultiOutputRegressor"
    write(file_num,'(A)') "import os"
    write(file_num,'(A)') ""

  End Subroutine Py_import

  Subroutine SK2F(self, file_num)
    Implicit None
    Class(Generate_Python) :: self
    Integer :: file_num
    write(file_num,'(A)') "def sk2f(write_coef):"
    write(file_num,'(A)') "    if (write_coef.type == 'Neural_Network'):"
    write(file_num,'(A)') "        nn_sk2f(write_coef)"
    write(file_num,'(A)') "    elif (write_coef.type == 'Random_Forest'):"
    write(file_num,'(A)') "        rf_sk2f(write_coef)"
    write(file_num,'(A)') "    elif (write_coef.type == 'Decision_Tree'):"
    write(file_num,'(A)') "        dt_sk2f(write_coef)"
    write(file_num,'(A)') "    else:"
    write(file_num,'(A)') "        print('wrong type')"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "def nn_sk2f(nn):"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "    file_name = 'nn_coef.dat'"
    write(file_num,'(A)') "    nn_output  = open(nn.output_coef_path+file_name, 'w')"
    write(file_num,'(A)') "    nn_output.write('! n_layers\n%d\n'%(nn.n_layers_))"
    write(file_num,'(A)') "    nn_output.write('! layer_sizes\n')"
    write(file_num,'(A)') "    nn_output.write('%d \n'%np.shape(nn.coefs_[0])[0])"
    write(file_num,'(A)') "    for i in range(len(nn.hidden_layer_sizes)):"
    write(file_num,'(A)') "        nn_output.write('%d '%nn.hidden_layer_sizes[i])"
    write(file_num,'(A)') "    nn_output.write('%d\n'%nn.n_outputs_)"
    write(file_num,'(A)') "    nn_output.write('! intercepts\n')"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "    for i in range(len(nn.intercepts_)):"
    write(file_num,'(A)') "        for j in range(len(nn.intercepts_[i])):"
    write(file_num,'(A)') "            nn_output.write('%f '%nn.intercepts_[i][j])"
    write(file_num,'(A)') "        nn_output.write('\n')"
    write(file_num,'(A)') "    nn_output.write('! coefs\n')"
    write(file_num,'(A)') "    for i in range(len(nn.coefs_)):"
    write(file_num,'(A)') "        nn_output.write('!! layer%d\n'%i)"
    write(file_num,'(A)') "        coef = np.transpose(nn.coefs_[i])"
    write(file_num,'(A)') "        for j in range(len(coef)):"
    write(file_num,'(A)') "            for k in range(len(coef[j])):"
    write(file_num,'(A)') "                nn_output.write('%f '%coef[j][k])"
    write(file_num,'(A)') "            nn_output.write('\n')"
    write(file_num,'(A)') "    nn_output.write('! activations\n')"
    write(file_num,'(A)') "    nn_output.write(nn.activation)"
    write(file_num,'(A)') "    nn_output.write('\n')"
    write(file_num,'(A)') "    nn_output.write('! out_activations\n')"
    write(file_num,'(A)') "    nn_output.write(nn.out_activation_)"
    write(file_num,'(A)') "    nn_output.write('\n')"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "def dt_sk2f(dt):"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "    dt_output = open(dt.output_coef_path+'dt_coef.dat','w')"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "    dt_output.write('! node_count\n')"
    write(file_num,'(A)') "    dt_output.write('%d\n'%dt.tree_.node_count)"
    write(file_num,'(A)') "    dt_output.write('! n_features\n')"
    write(file_num,'(A)') "    dt_output.write('%d\n'%dt.tree_.n_features)"
    write(file_num,'(A)') "    dt_output.write('! n_outputs\n')"
    write(file_num,'(A)') "    dt_output.write('%d\n'%dt.tree_.n_outputs)"
    write(file_num,'(A)') "    dt_output.write('! max_depth\n')"
    write(file_num,'(A)') "    dt_output.write('%d\n'%dt.tree_.max_depth)"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "    for i in range(dt.tree_.node_count):"
    write(file_num,'(A)') "        dt_output.write('! node %d\n'%(i+1))"
    write(file_num,'(A)') "        dt_output.write('%d\n'%(dt.tree_.children_left[i]+1))"
    write(file_num,'(A)') "        dt_output.write('%d\n'%(dt.tree_.children_right[i]+1))"
    write(file_num,'(A)') "        dt_output.write('%d\n'%(dt.tree_.feature[i]+1))"
    write(file_num,'(A)') "        dt_output.write('%f\n'%dt.tree_.threshold[i])"
    write(file_num,'(A)') "        for j in range(dt.tree_.n_outputs):"
    write(file_num,'(A)') "            dt_output.write('%f '%dt.tree_.value[i,j])"
    write(file_num,'(A)') "        dt_output.write('\n')"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "def rf_sk2f(rf):"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "    rf_output = open(rf.output_coef_path+'rf_coef.dat','w')"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "    rf_output.write('! tree_count\n')"
    write(file_num,'(A)') "    rf_output.write('%d\n'%len(rf.estimators_))"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "    trees = rf.estimators_"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "    for j in range(len(rf.estimators_)):"
    write(file_num,'(A)') "        tree1 = trees[j].tree_"
    write(file_num,'(A)') "        rf_output.write('! node_count\n')"
    write(file_num,'(A)') "        rf_output.write('%d\n'%tree1.node_count)"
    write(file_num,'(A)') "        rf_output.write('! n_features\n')"
    write(file_num,'(A)') "        rf_output.write('%d\n'%tree1.n_features)"
    write(file_num,'(A)') "        rf_output.write('! n_outputs\n')"
    write(file_num,'(A)') "        rf_output.write('%d\n'%tree1.n_outputs)"
    write(file_num,'(A)') "        rf_output.write('! max_depth\n')"
    write(file_num,'(A)') "        rf_output.write('%d\n'%tree1.max_depth)"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "        for i in range(tree1.node_count):"
    write(file_num,'(A)') "            rf_output.write('! node %d\n'%(i+1))"
    write(file_num,'(A)') "            rf_output.write('%d\n'%(tree1.children_left[i]+1))"
    write(file_num,'(A)') "            rf_output.write('%d\n'%(tree1.children_right[i]+1))"
    write(file_num,'(A)') "            rf_output.write('%d\n'%(tree1.feature[i]+1))"
    write(file_num,'(A)') "            rf_output.write('%f\n'%tree1.threshold[i])"
    write(file_num,'(A)') "            for j in range(tree1.n_outputs):"
    write(file_num,'(A)') "                rf_output.write('%f '%tree1.value[i,j])"
    write(file_num,'(A)') "            rf_output.write('\n')"
    write(file_num,'(A)') ""
    write(file_num,'(A)') ""

  End Subroutine SK2F
  !↑↑↑↑↑↑↑↑↑↑↑↑↑End generating python file↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑

 Subroutine NN_Load_Default_Para(self)
    Implicit None
    Class(NN_Parameters) :: self

    self%key(1)= 'hidden_layer_sizes'
    self%value(1)= "(100,)"

    self%key(2) = 'activation'
    self%value(2) = "'relu'"

    self%key(3) = 'solver'
    self%value(3) = "'adam'"

    self%key(4) = 'alpha'
    self%value(4) = "0.0001"

    self%key(5) = 'batch_size'
    self%value(5) = "'auto'"

    self%key(6) = 'learning_rate'
    self%value(6) = "'constant'"

    self%key(7) = 'learning_rate_init'
    self%value(7) = "0.001"

    self%key(8) = 'power_t'
    self%value(8) = "0.5"

    self%key(9) = 'max_iter'
    self%value(9) = "200"

    self%key(10) = 'shuffle'
    self%value(10) = "True"

    self%key(11) = 'random_state'
    self%value(11) = "None"

    self%key(12) = 'tol'
    self%value(12) = "0.0001"

    self%key(13) = 'verbose'
    self%value(13) = "False"

    self%key(14) = 'warm_start'
    self%value(14) = "False"

    self%key(15) = 'momentum'
    self%value(15) = "True"

    self%key(16) = 'nesterovs_momentum'
    self%value(16) = "True"

    self%key(17) = 'early_stopping'
    self%value(17) = "False"

    self%key(18) = "validation_fraction"
    self%value(18) = "0.1"

    self%key(19) = 'beta_1'
    self%value(19) = "0.9"

    self%key(20) = 'beta_2'
    self%value(20) = "0.999"

    self%key(21) = 'epsilon'
    self%value(21) = "1e-08"

  end Subroutine NN_Load_Default_Para

  Subroutine NN_Read_and_Update_Para(self, file_num)
    Implicit None

    Class(NN_Parameters) :: self
    Integer, Intent(In) :: file_num
    Character(50) :: hidden_layer_sizes = "NULL"
    Character(50) :: activation = "NULL"
    Character(50) :: solver = "NULL"
    Character(50) :: alpha = "NULL"
    Character(50) :: batch_size = "NULL"
    Character(50) :: learning_rate = "NULL"
    Character(50) :: learning_rate_init = "NULL"
    Character(50) :: power_t = "NULL"
    Character(50) :: max_iter = "NULL"
    Character(50) :: shuffle = "NULL"
    Character(50) :: random_state = "NULL"
    Character(50) :: tol = "NULL"
    Character(50) :: verbose = "NULL"
    Character(50) :: warm_start = 'NULL'
    Character(50) :: momentum = "NULL"
    Character(50) :: nesterovs_momentum = "NULL"
    Character(50) :: early_stopping = "NULL"
    Character(50) :: validation_fraction = "NULL"
    Character(50) :: beta_1 = "NULL"
    Character(50) :: beta_2 = "NULL"
    Character(50) :: epsilon = "NULL"

    Namelist /NN_Parameter/ hidden_layer_sizes, &
        activation, &
        solver, &
        alpha, &
        batch_size, &
        learning_rate, &
        learning_rate_init, &
        power_t, &
        max_iter, &
        shuffle, &
        random_state, &
        tol, &
        verbose, &
        warm_start, &
        momentum, &
        nesterovs_momentum, &
        early_stopping, &
        validation_fraction, &
        beta_1, &
        beta_2, &
        epsilon

    Call self%NN_Load_Default_Para

    Read(file_num, nml = NN_Parameter)

    If (hidden_layer_sizes  .ne. 'NULL') Then
      self%value(1) = hidden_layer_sizes 
    End If

    If (activation .ne. 'NULL') Then
      self%value(2) = activation
    End If

    If (solver .ne. 'NULL') Then
      self%value(3) = solver
    End If

    If (alpha .ne. 'NULL') Then
      self%value(4) = alpha
    End If

    If (batch_size .ne. 'NULL') Then
      self%value(5) = batch_size
    End If

    If (learning_rate .ne. 'NULL') Then
      self%value(6) = learning_rate
    End If

    If (learning_rate_init .ne. 'NULL') Then
      self%value(7) = learning_rate_init
    End If

    If (power_t .ne. 'NULL') Then
      self%value(8) = power_t
    End If

    If (max_iter .ne. 'NULL') Then
      self%value(9) = max_iter
    End If

    If (shuffle .ne. 'NULL') Then
      self%value(10) = shuffle
    End If

    If (random_state .ne. 'NULL') Then
      self%value(11) = random_state
    End If

    If (tol .ne. 'NULL') Then
      self%value(12) = tol
    End If

    If (verbose .ne. 'NULL') Then
      self%value(13) = verbose
    End If

    If (warm_start .ne. 'NULL') Then
      self%value(14) = warm_start
    End If

    If (momentum .ne. 'NULL') Then
      self%value(15) = momentum
    End If

    If (nesterovs_momentum .ne. 'NULL') Then
      self%value(16) = nesterovs_momentum
    End If

    If (early_stopping .ne. 'NULL') Then
      self%value(17) = early_stopping
    End If

    If (validation_fraction .ne. 'NULL') Then
      self%value(18) = validation_fraction
    End If

    If (beta_1 .ne. 'NULL') Then
      self%value(19) = beta_1
    End If

    If (beta_2 .ne. 'NULL') Then
      self%value(20) = beta_2
    End If

    If (epsilon .ne. 'NULL') Then
      self%value(21) = epsilon
    End If

  end Subroutine NN_Read_and_Update_Para

 Subroutine DT_Load_Default_Para(self)
    Implicit None
    Class(DT_Parameters) :: self

    self%key(1) = "criterion"
    self%value(1) = "'mse'"

    self%key(2) = "splitter"
    self%value(2) = "'best'"

    self%key(3) = "max_depth"
    self%value(3) = "None"

    self%key(4) = "min_samples_split"
    self%value(4) = "2 "

    self%key(5) = "min_samples_leaf"
    self%value(5) = "1"

    self%key(6) = "min_weight_fraction_leaf"
    self%value(6) = "0.0"

    self%key(7) = "max_features"
    self%value(7) = "None"

    self%key(8) = "random_state"
    self%value(8) = "None"

    self%key(9) = "max_leaf_nodes"
    self%value(9) = "None"

    self%key(10) = "min_impurity_decrease"
    self%value(10) = "0.0"

    self%key(11) = "min_impurity_split"
    self%value(11) = "None"

    self%key(12) = "presort "
    self%value(12) = "False"

  end Subroutine DT_Load_Default_Para

  Subroutine DT_Read_and_Update_Para(self, file_num)
    Implicit None

    Class(DT_Parameters) :: self
    Integer, intent(in) :: file_num
    Character(50) :: criterion ='NULL'
    Character(50) :: splitter ='NULL'
    Character(50) :: max_depth = 'NULL'
    Character(50) :: min_samples_split = 'NULL'
    Character(50) :: min_samples_leaf = 'NULL'
    Character(50) :: min_weight_fraction_leaf = 'NULL'
    Character(50) :: max_features = 'NULL'
    Character(50) :: random_state = 'NULL'
    Character(50) :: max_leaf_nodes ='NULL'
    Character(50) :: min_impurity_decrease = 'NULL'
    Character(50) :: min_impurity_split = 'NULL'
    Character(50) :: presort = 'NULL'

    Namelist /DT_Parameter/ criterion , &
        splitter , &
        max_depth , &
        min_samples_split , &
        min_samples_leaf , &
        min_weight_fraction_leaf , &
        max_features , &
        random_state , &
        max_leaf_nodes , &
        min_impurity_decrease , &
        min_impurity_split , &
        presort 

    Call self%DT_Load_Default_Para

    Read(file_num, nml = DT_Parameter)

    If (criterion  .ne. 'NULL') Then
      self%value(1) = criterion 
    End If

    If (splitter  .ne. 'NULL') Then
      self%value(2) = splitter 
    End If

    If (max_depth  .ne. 'NULL') Then
      self%value(3) = max_depth 
    End If

    If (min_samples_split  .ne. 'NULL') Then
      self%value(4) = min_samples_split 
    End If

    If (min_samples_leaf  .ne. 'NULL') Then
      self%value(5) = min_samples_leaf 
    End If

    If (min_weight_fraction_leaf  .ne. 'NULL') Then
      self%value(6) = min_weight_fraction_leaf 
    End If

    If (max_features  .ne. 'NULL') Then
      self%value(7) = max_features 
    End If

    If (random_state  .ne. 'NULL') Then
      self%value(8) = random_state 
    End If

    If (max_leaf_nodes  .ne. 'NULL') Then
      self%value(9) = max_leaf_nodes 
    End If

    If (min_impurity_decrease  .ne. 'NULL') Then
      self%value(10) = min_impurity_decrease 
    End If

    If (min_impurity_split  .ne. 'NULL') Then
      self%value(11) = min_impurity_split 
    End If

    If (presort  .ne. 'NULL') Then
      self%value(12) = presort 
    End If


  end Subroutine DT_Read_and_Update_Para

 Subroutine RF_Load_Default_Para(self)
    Implicit None
    Class(RF_Parameters) :: self

    self%key(1) = "n_estimators"
    self%value(1) = "'warn' "

    self%key(2) = "criterion"
    self%value(2) = "'mse' "

    self%key(3) = "max_depth"
    self%value(3) = "None"

    self%key(4) = "min_samples_split"
    self%value(4) = "2 "

    self%key(5) = "min_samples_leaf"
    self%value(5) = "1"

    self%key(6) = "min_weight_fraction_leaf"
    self%value(6) = "0.0 "

    self%key(7) = "max_features"
    self%value(7) = "'auto' "

    self%key(8) = "max_leaf_nodes"
    self%value(8) = "None"

    self%key(9) = "min_impurity_decrease"
    self%value(9) = "0.0 "

    self%key(10) = "min_impurity_split"
    self%value(10) = "None"

    self%key(11) = "bootstrap"
    self%value(11) = "True "

    self%key(12) = "oob_score"
    self%value(12) = "False "

    self%key(13) = "n_jobs"
    self%value(13) = "None"

    self%key(14) = "random_state"
    self%value(14) = "None "

    self%key(15) = "verbose"
    self%value(15) = "0 "

    self%key(16) = "warm_start"
    self%value(16) = "False"

  End Subroutine RF_Load_Default_Para

  Subroutine RF_Read_and_Update_Para(self, file_num)
    Implicit None

    Class(RF_Parameters) :: self
    Integer, intent(in) :: file_num

    Character(50) :: n_estimators= 'NULL'
    Character(50) :: criterion= 'NULL'
    Character(50) :: max_depth= 'NULL'
    Character(50) :: min_samples_split= 'NULL'
    Character(50) :: min_samples_leaf= 'NULL'
    Character(50) :: min_weight_fraction_leaf= 'NULL'
    Character(50) :: max_features= 'NULL'
    Character(50) :: max_leaf_nodes= 'NULL'
    Character(50) :: min_impurity_decrease= 'NULL'
    Character(50) :: min_impurity_split= 'NULL'
    Character(50) :: bootstrap= 'NULL'
    Character(50) :: oob_score= 'NULL'
    Character(50) :: n_jobs= 'NULL'
    Character(50) :: random_state= 'NULL'
    Character(50) :: verbose= 'NULL'
    Character(50) :: warm_start= 'NULL'

    Namelist /RF_Parameter/ n_estimators, &
        criterion, &
        max_depth, &
        min_samples_split, &
        min_samples_leaf, &
        min_weight_fraction_leaf, &
        max_features, &
        max_leaf_nodes, &
        min_impurity_decrease, &
        min_impurity_split, &
        bootstrap, &
        oob_score, &
        n_jobs, &
        random_state, &
        verbose, &
        warm_start

    Call self%RF_Load_Default_Para

    Read(file_num, nml = RF_Parameter)

    If (n_estimators .ne. 'NULL') Then
      self%value(1) = n_estimators
    End If

    If (criterion .ne. 'NULL') Then
      self%value(2) = criterion
    End If

    If (max_depth .ne. 'NULL') Then
      self%value(3) = max_depth
    End If

    If (min_samples_split .ne. 'NULL') Then
      self%value(4) = min_samples_split
    End If

    If (min_samples_leaf .ne. 'NULL') Then
      self%value(5) = min_samples_leaf
    End If

    If (min_weight_fraction_leaf .ne. 'NULL') Then
      self%value(6) = min_weight_fraction_leaf
    End If

    If (max_features .ne. 'NULL') Then
      self%value(7) = max_features
    End If

    If (max_leaf_nodes .ne. 'NULL') Then
      self%value(8) = max_leaf_nodes
    End If

    If (min_impurity_decrease .ne. 'NULL') Then
      self%value(9) = min_impurity_decrease
    End If

    If (min_impurity_split .ne. 'NULL') Then
      self%value(10) = min_impurity_split
    End If

    If (bootstrap .ne. 'NULL') Then
      self%value(11) = bootstrap
    End If

    If (oob_score .ne. 'NULL') Then
      self%value(12) = oob_score
    End If

    If (n_jobs .ne. 'NULL') Then
      self%value(13) = n_jobs
    End If

    If (random_state .ne. 'NULL') Then
      self%value(14) = random_state
    End If

    If (verbose .ne. 'NULL') Then
      self%value(15) = verbose
    End If

    If (warm_start .ne. 'NULL') Then
      self%value(16) = warm_start
    End If

  end Subroutine RF_Read_and_Update_Para

  Subroutine Gen_Training_Param(self, file_num, Training_type)
    Implicit None
    Class(Generate_Python) :: self
    Integer, intent(in) :: file_num
    Character(20) :: Training_type
    Character(1000) :: Temp
    Integer :: i

    self%Training_type = Training_type

    If (self%Training_type .eq. 'Neural_Network') Then
      Call self%NN_Para%NN_Read_and_Update_Para(file_num)
      Temp = 'ml = MLPRegressor('
      Do i = 1, self%NN_para%num_para - 1
        Temp = Trim(Temp)//Trim(self%NN_Para%key(i)) // &
            ' = ' // Trim(self%NN_Para%value(i)) // ','
      End Do
      i = self%NN_para%num_para
      Temp = Trim(Temp)//Trim(self%NN_Para%key(i)) // &
          ' = ' // Trim(self%NN_Para%value(i)) // ')'
    Else If (self%Training_type .eq. 'Decision_Tree') Then
      Call self%DT_Para%DT_Read_and_Update_Para(file_num)

      Temp = 'ml = tree.DecisionTreeRegressor('
      Do i = 1, self%DT_para%num_para - 1
        Temp = Trim(Temp)//Trim(self%DT_Para%key(i)) // &
            ' = ' // Trim(self%DT_Para%value(i)) // ','
      End Do
      i = self%DT_para%num_para
      Temp = Trim(Temp)//Trim(self%DT_Para%key(i)) // &
          ' = ' // Trim(self%DT_Para%value(i)) // ')'
    Else If (self%Training_type .eq. 'Random_Forest') Then
      Call self%RF_Para%RF_Read_and_Update_Para(file_num)

      Temp = 'ml = RandomForestRegressor('
      Do i = 1, self%RF_para%num_para - 1
        Temp = Trim(Temp)//Trim(self%RF_Para%key(i)) // &
            ' = ' // Trim(self%RF_Para%value(i)) // ','
      End Do
      i = self%RF_para%num_para
      Temp = Trim(Temp)//Trim(self%RF_Para%key(i)) // &
          ' = ' // Trim(self%RF_Para%value(i)) // ')'
    End If
    self%Training_script = Temp

    print *, temp

  End Subroutine Gen_Training_Param


End Module Mod_Fsklearn_Essential
