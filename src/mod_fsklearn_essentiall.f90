!---------------------------------------------------------
! mod_fsklearn_essential
!---------------------------------------------------------
!
!   - In this module, some essential parts for the fsklearn
!     are defined. They include three types of essential
!     functions, some files and some derived type variables.
!  - They will be used in mod_fsklearn to build the full
!     machine learning module.
!
!--------------------------------------------------------
! Interfaces:
!--------------------------------------------------------
!
!   - Include the following components:
!   - 1) Read_XX:
!     + Read_Neural_Network
!     + Read_Decision_Tree
!     + Read_Random_Forest
!   - 2) Predict_XX:
!     + Predict_Neural_Network
!     + Predict_Decision_Tree
!     + Predict_Random_Forest
!   - 3) Activation_XX:
!     + Activation_logistic
!     + Activation_tanh
!     + Activation_ReLU
!     + Activation_identity
!     + Activation_softmax
!  - For more details, see the corresponding region for
!    each function.
!
!--------------------------------------------------------
! Files:
!--------------------------------------------------------
!
! - Files associate with Fsklearn. If not set, the path
!   of the training data and the training parameters
!   will be "fsklearn_data" and "fsklearn_para" respectively
!
! - By dedault, only the "dsklearn_data", "fsklearn_para"
!   needed to be set in fsklearn_initialization.
!
! - "para_files_path":
!   + the path for fsklearn related parameters
!
! - "set_ML_file":
!   + File name for setting parameters
!   + Located in the path "para_files_path"
!   + need to assign input vector length, output vector
!     length, training parameters and some other
!     custom parameters
!
! - "f2py_training_param":
!   + json file that passes the parameter to Python
!   + Located in the path "para_files_path"
!
! - "nn_param.dat":
!   + Neural network parameters from Python code
!   + Located in the path "para_files_path"
!
! - "dt_param.dat":
!   + Decision tree parameters from Python code
!   + Located in the path "para_files_path"
!
! - "rf_param.dat":
!   + Random forest parameters from Python code
!   + Located in the path "para_files_path"
!
! - "training_data_path":
!   + the path for training data
!
! - "training_py":
!   + python code generated by Fortran code
!
! - "training_data_path":
!   + the path for training data
! -"training_input_name":
!   + File name for input data
!   + Located in the path "training_data_path"
!   + The length of the data in each row is n_inputs
!   + If mpi with N processot is defined, there will be
!     N input files, Then all files will be gathered
!     automatically by the Python code.
!
! - "training_output_name":
!   + Located in the path "training_data_path"
!   + file name for put put data
!   + The length of the data in each row is n_outputs
!     N input files, Then all files will be gathered
!     automatically by the Python code.
!
!--------------------------------------------------------
! Module variables:
!--------------------------------------------------------
!
! PS:
!    - Precision for float number.
!    - PS = 4: single precision
!    - PS = 8: double precision
!
! Fsklearn_Define:
!    - Derived type that contains the machine learning
!       variables. It can do training and prediction
!       with a uniform way.
!    - %n_inputs:
!       + length of the input vector
!       + It is necessary to provide the length of the
!         input vector during initialization.
!    - %n_outputs:
!       + length of the output vector
!       + It is necessary to provide the length of the
!         output vector during initialization.
!    - %Inputs:
!       + Input vector, not necessary
!    - %Outputs:
!       + Output vector, not necessary
!    - %Para_Read:
!       + Read the parameters and coefficients during
!         initialization for PREDICTION.
!    - %Predict:
!       + Predict procedure
!    - %Gen_Training:
!       + Procedure for generate training data, can be
!         point to other subroutines.
!    - %Predict:
!       + Procedure for initialization, can be point to
!         other subroutines.
!
! Ragged_Vector:
!    - Declaration of ragged vector and ragged matrix.
!    - In this case, designed for neural networks.
!    - Ragged vectors for 2-D array consists of vectors with
!      different length
!    - Ragged vectors for 3-D array consists of matrice with
!      different sized
!
! NN_activation:
!    - Procedure used to choose activation function
!      at the beginning.
!
! Neural_Network:
!    - Derived Type containing all Neural network variables.
!    - %input_len:
!       + length of the input vector
!    - %output_len:
!       + length of the output vector
!    - %layers:
!       + layer of neural network
!    - %layer_size:
!       + 1-D vector with $(%layers) length, each element
!         each element represents the represents the number
!         of the neural in the corresponding layer.
!    - %Activations:
!       + Activation functions for each layer
!       + Ragged vector with $(%layers)-1 vectors, the length
!         of each vector Activations(i)%Vec is the same as
!         $(layer_size(i))
!    - %Intercepts:
!       + bias variable for each layer
!       + Ragged vector with $(%layers)-1 vectors, the length
!         of each vector Intercepts(i)%Vec is the same as
!         $(layer_size(i))
!    - %Coefs:
!       + coefficient for each neural
!       + Ragged matrix with $(%layers)-1 matrices
!       + The size Coefs(i)%Mat is
!         [$(layer_size(i+1)),$(layer_size(i))]
!    - %Activation type:
!       + Activation function. Contains 5 types:
!        'sigmoid', 'tanh', 'ReLU', 'ideneity', 'softmax'
!    - %Out_activation type:
!       + Activation function for output. Contains 5 types:
!        'sigmoid', 'tanh', 'ReLU', 'ideneity', 'softmax'
!    - %Activation:
!       + Uniform procedure calling activation function
!    - %Out_activation:
!       + Uniform procedure calling out activation function
!    - %Para_Read:
!       + Uniform procedure reading Neural Network parameters
!    - %Predict:
!       + Uniform procedure predicting Neural Network results
!
! Nodes:
!    - Derived type for binary tree node class
!    - Used in decision tree and random forest method
!    - %children_left:
!       + Node number of the left child
!    - %children_right:
!       + Node number of the Right child
!    - %feature:
!       + Choice of the input feature for logistic operation
!    - %threshold:
!       + Threshold for logistic operation
!    - %Value:
!       + Value of the predicted result for the corresponding
!         node. with the length of $(n_outputs)
!
! Decision_Tree:
!    - Derived Type containing all Decision Tree variables.
!    - %node_count
!       + amount of the nodes
!    - %n_inputs
!       + length of the input variables
!    - %n_outputs
!       + length of the output variables
!    - %max_depth
!       + maximum depth of the tree
!    - %Node
!       + nodes in decision trees, Nodes type
!
! Random_Forest:
!    - Derived Type containing all Random_Forest variables.
!    - %tree_count
!       + amount of the decision treees
!    - %n_inputs
!       + length of the input variables
!    - %n_outputs
!       + length of the output variables
!    - %Trees
!       + trees in random forest, Decision_Tree type
!
! by Zhouteng Ye
! Last update: 04/17/2019
!
!---------------------------------------------------------

Module Mod_Fsklearn_Essential

  ! Subject to the choice of precision.
  ! Single precision by default
# if defined(DOUBLE_PRECISION)
  Integer, Private, Parameter :: PS = 8
# else
  Integer, Private, Parameter :: PS = 4
# endif

  Type Ragged_Vector
    Real(PS), Allocatable :: Vec(:)
  End Type Ragged_vector
  Type Ragged_Matrix
    Real(PS), Allocatable :: Mat(:,:)
  End Type Ragged_Matrix

  !↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓Neural Networks variables↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓
  Type :: NN_activation
    Procedure(Sub_Interface), Pointer, NoPass :: activate => NULL()
  End Type NN_activation

  ! interface for choose activation type
  Interface
    Function Sub_Interface(n, X)
      Import :: PS
      Integer,  Intent(in) :: n
      Real(PS), Intent(in), Dimension(n) :: X
      Real(PS), Dimension(n) :: Sub_Interface
    End Function Sub_Interface
  End Interface

  ! neural network parameters
  Type :: Neural_Network
    Integer :: input_len
    Integer :: output_len
    Integer :: layers
    Integer, Allocatable :: Layer_Size(:)
    Type(Ragged_Vector), Allocatable :: Activations(:)
    Type(Ragged_Vector), Allocatable :: Intercepts(:)
    Type(Ragged_Matrix), Allocatable :: Coefs(:)
    Character(10) :: Activation_type
    Character(10) :: out_Activation_type
    Type(NN_Activation) :: Activation
    Type(NN_Activation) :: Out_Activation
  Contains
    Procedure , NoPass :: Para_Read => Read_Neural_Network
    Procedure , NoPass :: Predict   => Predict_Neural_Network
  End Type Neural_Network
  !↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑End Neural Network Variables↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑


  !↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓Decision Tree Variables↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓
  Type Nodes
    Integer  :: children_left
    Integer  :: children_right
    Integer  :: feature
    Real(PS) :: threshold
    Real(PS), Allocatable :: Values(:)
  End Type Nodes

  Type:: Decision_Tree
    Integer :: node_count
    Integer :: n_inputs
    Integer :: n_outputs
    Integer :: max_depth
    Type(Nodes), Allocatable :: Node(:)
  Contains
    Procedure , NoPass :: Para_Read => Read_Decision_Tree
    Procedure , NoPass :: Predict => Predict_Decision_Tree
  End Type Decision_Tree
  !↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑End Decision Tree Variables↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑


  !↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓Random Forest Variables↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓
  Type:: Random_Forest
    Integer :: tree_count
    Integer :: n_inputs
    Integer :: n_outputs
    Type(Decision_Tree), Allocatable :: Trees(:)
  Contains
    Procedure , NoPass :: Para_Read => Read_Random_Forest
    Procedure , NoPass :: Predict => Predict_Random_Forest
  End Type Random_Forest
  !↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑End Random Forest Variables↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑

  !↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓Files↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓
  Character(100) :: para_files_path
  Character(100) :: set_ML_file         = 'fsklearn_param.namelist'
  Character(100) :: f2py_training_param = 'training_param.json'
  Character(100) :: nn_param_name       = 'nn_param.dat'
  Character(100) :: dt_param_name       = 'dt_param.dat'
  Character(100) :: rf_param_name       = 'rf_param.dat'
  Character(100) :: training_py         = 'training.py'

  Character(100) :: training_data_path
  Character(100) :: traing_input_name   = 'training_input'
  Character(100) :: traing_output_name  = 'training_output'
  !↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑End Files↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑

  ! Predefined type variables
  Type(Neural_Network) :: N_Work
  Type(Decision_Tree) :: D_Tree
  Type(Random_Forest) :: R_Forest

Contains

  Subroutine Read_Neural_Network

# if defined (PARALLEL)
    Use mpi
# endif
    Implicit None

    Integer :: i,j
    Integer :: error
    Character(100) :: string

    Character :: activation
    Character :: out_activation
    character(100) :: tmp, tmp1
    Type(Decision_Tree) :: tree1
    Integer :: n_proc
    Integer :: myid
    Character(10) :: string_myid
    Integer :: ier

    tmp = Trim(adjustl(para_files_path))// &
        Trim(adjustl(nn_param_name))

# if defined (PARALLEL)
! Parallel version
    ! ugly mpi implementation here
    Call MPI_COMM_RANK(MPI_COMM_WORLD, myid, ier)
    Call MPI_COMM_SIZE(MPI_COMM_WORLD, n_proc, ier)
    Write(string_myid,"(I10)") myid
    tmp1 = Trim(Adjustl(tmp))// &
        Trim(Adjustl(string_myid))
    Do i = 0, n_proc-1
      If (myid .eq. i) Then
        Call Execute_Command_Line('cp '//tmp//' '//tmp1)
      End If
    End Do

    open(4000+myid, file=tmp,status='unknown')

    Read(4000+myid, *,iostat=error) string
    Read(4000+myid, *) N_Work%layers
    Allocate(N_Work%layer_size(N_Work%layers))

    Read(4000+myid, *,iostat=error) string
    Read(4000+myid, *) N_Work%layer_size
    N_Work%input_len = N_Work%layer_size(1)
    N_Work%output_len = N_Work%layer_size(N_Work%layers)

    Allocate(N_Work%activations(N_Work%layers))
    Do i = 1,N_Work%layers
      Allocate(N_Work%activations(i)%vec(N_Work%layer_size(i)))
    End do

    Read(4000+myid, *,ostat=error) string
    Allocate(N_Work%intercepts(N_Work%layers-1))
    Do i = 1,N_Work%layers-1
      Allocate(N_Work%intercepts(i)%vec(N_Work%layer_size(i+1)))
      Read(4000+myid, *) N_Work%intercepts(i)%vec
    End do

    Read(4000+myid, *,iostat=error) string
    Allocate(N_Work%coefs(N_Work%layers-1))
    Do i = 1,N_Work%layers-1
      Allocate(N_Work%coefs(i)%mat(N_Work%layer_size(i+1),N_Work%layer_size(i)))
      Read(4000+myid, *,iostat=error) string
      Do j = 1,N_Work%layer_size(i+1)
        Read(4000+myid, *) N_Work%coefs(i)%mat(j,:)
      End do
    End do

    Read(4000+myid, *,iostat=error) string
    Read(4000+myid, *) N_Work%Activation_type
    Read(4000+myid, *,iostat=error) string
    Read(4000+myid, *) N_Work%out_Activation_type

    Close(81)

    if (Trim(N_Work%Activation_type).eq.'logistic') Then
      N_Work%activation%activate => Activation_logistic
    else if (Trim(N_Work%Activation_type).eq.'tanh') Then
      N_Work%activation%activate => Activation_tanh
    else if (Trim(N_Work%Activation_type).eq.'softmax') Then
      N_Work%activation%activate => Activation_softmax
    else if (Trim(N_Work%Activation_type).eq.'relu') Then
      N_Work%activation%activate => Activation_ReLU
    else if (Trim(N_Work%Activation_type).eq.'identity') Then
      N_Work%activation%activate => Activation_identity
    else
      write(*,*) 'invalid activation type'
      Stop
    End if

    If (Trim(N_Work%out_Activation_type).eq.'logistic') Then
      N_Work%out_activation%activate => Activation_logistic
    Else If (Trim(N_Work%out_Activation_type).eq.'tanh') Then
      N_Work%out_activation%activate => Activation_tanh
    Else If (Trim(N_Work%out_Activation_type).eq.'softmax') Then
      N_Work%out_activation%activate => Activation_softmax
    Else If (Trim(N_Work%out_Activation_type).eq.'relu') Then
      N_Work%out_activation%activate => Activation_ReLU
    Else If (Trim(N_Work%out_Activation_type).eq.'identity') Then
      N_Work%out_activation%activate => Activation_identity
    Else
      Write(*,*) 'invalid output activation type'
      Stop
    End if
# else
! sequential code

    open(81,file=tmp,status='unknown')

    Read(81,*,iostat=error) string
    Read(81,*) N_Work%layers
    Allocate(N_Work%layer_size(N_Work%layers))

    Read(81,*,iostat=error) string
    Read(81,*) N_Work%layer_size
    N_Work%input_len = N_Work%layer_size(1)
    N_Work%output_len = N_Work%layer_size(N_Work%layers)


    Allocate(N_Work%activations(N_Work%layers))
    Do i = 1,N_Work%layers
      Allocate(N_Work%activations(i)%vec(N_Work%layer_size(i)))
    End do

    Read(81,*,iostat=error) string
    Allocate(N_Work%intercepts(N_Work%layers-1))
    Do i = 1,N_Work%layers-1
      Allocate(N_Work%intercepts(i)%vec(N_Work%layer_size(i+1)))
      Read(81,*) N_Work%intercepts(i)%vec
    End do


    Read(81,*,iostat=error) string
    Allocate(N_Work%coefs(N_Work%layers-1))
    Do i = 1,N_Work%layers-1
      Allocate(N_Work%coefs(i)%mat(N_Work%layer_size(i+1),N_Work%layer_size(i)))
      Read(81,*,iostat=error) string
      Do j = 1,N_Work%layer_size(i+1)
        Read(81,*) N_Work%coefs(i)%mat(j,:)
      End do
    End do

    Read(81,*,iostat=error) string
    Read(81,*) N_Work%Activation_type
    Read(81,*,iostat=error) string
    Read(81,*) N_Work%out_Activation_type

    Close(81)

    If (Trim(N_Work%Activation_type).eq.'logistic') Then
      N_Work%activation%Activate => Activation_logistic
    Else If (Trim(N_Work%Activation_type).eq.'tanh') Then
      N_Work%activation%Activate => Activation_tanh
    Else If (Trim(N_Work%Activation_type).eq.'softmax') Then
      N_Work%activation%Activate => Activation_softmax
    Else If (Trim(N_Work%Activation_type).eq.'relu') Then
      N_Work%activation%Activate => Activation_ReLU
    Else If (Trim(N_Work%Activation_type).eq.'identity') Then
      N_Work%activation%Activate => Activation_identity
    Else
      Write(*,*) 'invalid activation type'
    End If

    If (Trim(N_Work%out_Activation_type).eq.'logistic') Then
      N_Work%out_activation%Activate => Activation_logistic
    Else If (Trim(N_Work%out_Activation_type).eq.'tanh') Then
      N_Work%out_activation%Activate => Activation_tanh
    Else If (Trim(N_Work%out_Activation_type).eq.'softmax') Then
      N_Work%out_activation%Activate => Activation_softmax
    Else If (Trim(N_Work%out_Activation_type).eq.'relu') Then
      N_Work%out_activation%Activate => Activation_ReLU
    Else If (Trim(N_Work%out_Activation_type).eq.'identity') Then
      N_Work%out_activation%Activate => Activation_identity
    Else
      Write(*,*) 'invalid output activation type'
    End If

# endif

  End Subroutine Read_Neural_Network

  Subroutine Read_Decision_Tree
    
# if defined (PARALLEL)
    Use mpi
# endif
    Implicit None
    Integer :: i,j
    Integer :: error
    Character(20) :: string
    Character(100) :: tmp, tmp1
    Integer :: n_proc
    Integer :: myid
    Character(10) :: string_myid
    Integer :: ier

    tmp = Trim(Adjustl(para_files_path))// &
        Trim(Adjustl(dt_param_name)) 

# if defined (PARALLEL)
! Parallel version
    ! ugly mpi implementation here
    Call MPI_COMM_RANK(MPI_COMM_WORLD, myid, ier)
    Call MPI_COMM_SIZE(MPI_COMM_WORLD, n_proc, ier)
    Write(string_myid,"(I10)") myid
    tmp1 = Trim(Adjustl(tmp))// &
        Trim(Adjustl(string_myid))
    Do i = 0, n_proc-1
      If (myid .eq. i) Then
        Call Execute_Command_Line('cp '//tmp//' '//tmp1)
      End If
    End Do

    Open(4000+myid, file=tmp, status='unknown')

    ! tree related parameters
    Read(4000+myid,*,iostat=error) string
    Read(4000+myid,*) D_Tree%node_count
    Read(4000+myid,*,iostat=error) string
    Read(4000+myid,*) D_Tree%n_inputs
    Read(4000+myid,*,iostat=error) string
    Read(4000+myid,*) D_Tree%n_outputs
    Read(4000+myid,*,iostat=error) string
    Read(4000+myid,*) D_Tree%max_depth

    Allocate(D_Tree%node(D_TREE%node_count))

    ! binary tree
    Do i = 1,D_Tree%node_count
      Allocate(D_Tree%node(i)%values(D_TREE%n_outputs))
      Read(4000+myid,*,iostat=error) string
      Read(4000+myid,*) D_Tree%node(i)%children_left
      Read(4000+myid,*) D_Tree%node(i)%children_right
      Read(4000+myid,*) D_Tree%node(i)%feature
      Read(4000+myid,*) D_Tree%node(i)%threshold
      Read(4000+myid,*) D_Tree%node(i)%values
    End do

    Close(4000+myid)

    Do i = 0, n_proc-1
      If (myid .eq. i) Then
        Call Execute_Command_Line('rm '//tmp1)
      End If
    End Do

# else
! sequential version
    Open(82, file=tmp, status='unknown')

    Read(82,*,iostat=error) string
    Read(82,*) D_Tree%node_count
    Read(82,*,iostat=error) string
    Read(82,*) D_Tree%n_inputs
    Read(82,*,iostat=error) string
    Read(82,*) D_Tree%n_outputs
    Read(82,*,iostat=error) string
    Read(82,*) D_Tree%max_depth

    Allocate(D_Tree%node(D_TREE%node_count))

    Do i = 1,D_Tree%node_count
      Allocate(D_Tree%node(i)%values(D_TREE%n_outputs))
      Read(82,*,iostat=error) string
      Read(82,*) D_Tree%node(i)%children_left
      Read(82,*) D_Tree%node(i)%children_right
      Read(82,*) D_Tree%node(i)%feature
      Read(82,*) D_Tree%node(i)%threshold
      Read(82,*) D_Tree%node(i)%values
    End do

    Close(82)
# endif

  End Subroutine Read_Decision_Tree

  Subroutine Read_Random_Forest
# if defined(PARALLEL)
    Use mpi
#endif
    Implicit None
    Integer :: i,j
    Integer :: error
    Character(100) :: string
    character(100) :: tmp, tmp1
    Type(Decision_Tree) :: tree1
    Integer :: n_proc
    Integer :: myid
    Character(10) :: string_myid
    Integer :: ier

    tmp = Trim(Adjustl(para_files_path))// &
        Trim(Adjustl(rf_param_name)) 

# if defined (PARALLEL)
! Parallel version
    ! ugly mpi implementation here
    Call MPI_COMM_RANK(MPI_COMM_WORLD, myid, ier)
    Call MPI_COMM_SIZE(MPI_COMM_WORLD, n_proc, ier)
    Write(string_myid,"(I10)") myid
    tmp1 = Trim(Adjustl(tmp))// &
        Trim(Adjustl(string_myid))
    Do i = 0, n_proc-1
      If (myid .eq. i) Then
        Call Execute_Command_Line('cp '//tmp//' '//tmp1)
      End If
    End Do

    Open(4000+myid, file=tmp, status='unknown')
    Read(4000+myid, *, iostat=error) string
    Read(4000+myid, *) R_Forest%tree_count

    Allocate(R_Forest%trees(R_Forest%tree_count))

    ! Read from each decision trees
    Do j = 1, R_Forest%tree_count
      Read(4000+myid,*, iostat=error) string
      Read(4000+myid,*) R_Forest%trees(j)%node_count

      Read(4000+myid,*, iostat=error) string
      Read(4000+myid,*) R_Forest%trees(j)%n_inputs

      Read(4000+myid,*, iostat=error) string
      Read(4000+myid,*) R_Forest%trees(j)%n_outputs

      Read(4000+myid,*, iostat=error) string
      Read(4000+myid,*) R_Forest%trees(j)%max_depth

      Allocate(R_Forest%trees(j)%node(R_Forest%trees(j)%node_count))

      Do i = 1, R_Forest%trees(j)%node_count
        Allocate(R_Forest%trees(j)%node(i)%values(R_Forest%trees(j)%n_outputs))
        Read(4000+myid, *,iostat=error) string
        Read(4000+myid, *) R_Forest%trees(j)%node(i)%children_left
        Read(4000+myid, *) R_Forest%trees(j)%node(i)%children_right
        Read(4000+myid, *) R_Forest%trees(j)%node(i)%feature
        Read(4000+myid, *) R_Forest%trees(j)%node(i)%threshold
        Read(4000+myid, *) R_Forest%trees(j)%node(i)%values
      End Do
    End Do

    R_Forest%n_inputs  = R_Forest%trees(1)%n_inputs
    R_Forest%n_outputs = R_Forest%trees(1)%n_outputs

    Close(4000+myid)

    Do i = 0, n_proc-1
      If (myid .eq. i) Then
        Call Execute_Command_Line('rm '//tmp1)
      End If
    End Do

# else
!sequential version
    open(83,file=tmp,status='unknown')
    Read(83, *,iostat=error) string
    Read(83, *) R_Forest%tree_count

    Allocate(R_Forest%trees(R_Forest%tree_count))

    Do j = 1, R_Forest%tree_count
      Read(83,*,iostat=error) string
      Read(83,*) R_Forest%trees(j)%node_count
      Read(83,*,iostat=error) string
      Read(83,*) R_Forest%trees(j)%n_inputs
      Read(83,*,iostat=error) string
      Read(83,*) R_Forest%trees(j)%n_outputs
      Read(83,*,iostat=error) string
      Read(83,*) R_Forest%trees(j)%max_depth

      Allocate(R_Forest%trees(j)%node(R_Forest%trees(j)%node_count))

      Do i = 1,R_Forest%trees(j)%node_count
        Allocate(R_Forest%trees(j)%node(i)%values(R_Forest%trees(j)%n_outputs))
        Read(83,*,iostat=error) string
        Read(83,*) R_Forest%trees(j)%node(i)%children_left
        Read(83,*) R_Forest%trees(j)%node(i)%children_right
        Read(83,*) R_Forest%trees(j)%node(i)%feature
        Read(83,*) R_Forest%trees(j)%node(i)%threshold
        Read(83,*) R_Forest%trees(j)%node(i)%values
      End do
    End do

    R_Forest%n_inputs  = R_Forest%trees(1)%n_inputs
    R_Forest%n_outputs = R_Forest%trees(1)%n_outputs

    Close(83)
# endif

  End Subroutine read_Random_Forest

  !↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Predict Subroutines↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓
  function predict_Neural_Network(input, n_input, n_output)
    Implicit None
    integer :: n_input
    integer :: n_output
    Real(PS) :: input(n_input)
    Real(PS) :: predict_Neural_Network(n_output) 
    integer :: i

    N_Work%activations(1)%vec = input

    Do i = 1, N_Work%layers-2
      N_Work%activations(i+1)%vec = matmul(N_Work%coefs(i)%mat,N_Work%activations(i)%vec) + N_Work%intercepts(i)%vec
      N_Work%activations(i+1)%vec =N_Work%activation%activate(N_Work%layer_size(i+1),N_Work%activations(i+1)%vec)
    End do
    N_Work%activations(N_Work%layers)%vec = &
        matmul(N_Work%coefs(N_Work%layers-1)%mat,N_Work%activations(N_Work%layers-1)%vec) + N_Work%intercepts(N_Work%layers-1)%vec
    N_Work%activations(N_Work%layers)%vec = &
    N_Work%out_activation%activate(N_Work%output_len,N_Work%activations(N_Work%layers)%vec)

    predict_Neural_Network = N_Work%activations(N_Work%layers)%vec

  End function predict_Neural_Network

  function predict_Decision_Tree(input,n_input,n_output)
    Implicit None
    integer :: n_input
    integer :: n_output
    Real(PS) :: input(n_input)
    Real(PS) :: predict_Decision_Tree(n_output)

    integer :: i,n

    n = 1
    Do i = 1, D_Tree%max_depth
      if (D_Tree%node(n)%feature .eq. -1) Exit
      if (input(D_Tree%node(n)%feature) .le. D_TREE%node(n)%threshold) Then
        n = D_Tree%node(n)%children_left
      else
        n = D_Tree%node(n)%children_right
      End if
    End do

    predict_Decision_Tree = D_Tree%node(n)%values

  End function predict_Decision_Tree

  function predict_Random_Forest(input,n_input,n_output)
    Implicit None
    integer :: n_input
    integer :: n_output
    Real(PS) :: input(n_input)
    Real(PS) :: predict_Random_Forest(n_output)

    integer :: i, j, n

    predict_Random_Forest = 0.0_PS
    Do j = 1, R_Forest%tree_count
      n=1
      Do i = 1, R_Forest%trees(j)%max_depth
        if (R_Forest%trees(j)%node(n)%feature .eq. -1) Exit
        if (input(R_Forest%trees(j)%node(n)%feature) .le. R_Forest%trees(j)%node(n)%threshold) Then
          n = R_Forest%trees(j)%node(n)%children_left
        else
          n = R_Forest%trees(j)%node(n)%children_right
        End if
      End do
      predict_Random_Forest = predict_Random_Forest + R_Forest%trees(j)%node(n)%values
    End do

    predict_Random_Forest = predict_Random_Forest / R_Forest%tree_count

  End function predict_Random_Forest
  !↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑End Prediction Subroutines↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑


  !↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Generate_Training_Data↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓
  !-----------------------------------------------------
  ! In this part, the activation functions are defined.
  !   all the activation functions is called by a
  !   uniform procedure "Activation".
  !
  !-----------------------------------------------------
  ! I/O
  !-----------------------------------------------------
  !
  ! inputs:
  !   X:
  !     input vector, should be 1-D
  !   n:
  !     length of the input vector
  ! output:
  !    function result:
  !     output vector, should be 1-D with n length
  !
  !-----------------------------------------------------
  ! Activation functions
  !-----------------------------------------------------
  !
  ! Logistic function:
  !    f(x) = 1/(1+e^-x)
  !
  ! Tanh function:
  !    f(x) = tanh(x)
  !
  ! ReLU function:
  !    f(x) = max(0,x)
  !
  ! Identity function:
  !    f(x) = x
  !
  ! Softmax function:
  !    f(x)_i = exp(x_i)/sigma(x_i)
  !    Only used for classification.
  !    (Which means useless currently)
  !-----------------------------------------------------
  function Activation_logistic(n,X)
    Implicit None
    integer, Intent(in) :: n
    Real(PS), Intent(in), dimension(n) :: X
    Real(PS), dimension(n) :: Activation_logistic
    Activation_logistic = 1.0 / (1.0+exp(-X))
  End function Activation_logistic

  function Activation_tanh(n,X)
    Implicit None
    integer, Intent(in) :: n
    Real(PS), Intent(in), dimension(n) :: X
    Real(PS), dimension(n) :: Activation_tanh
    Activation_tanh = tanh(X)
  End function Activation_tanh

  function Activation_ReLU(n,X)
    Implicit None
    integer, Intent(in) :: n
    Real(PS), Intent(in), dimension(n) :: X
    Real(PS), dimension(n) :: Activation_ReLU
      Activation_ReLU = max(X,0.d0)
    End function Activation_ReLU

  function Activation_identity(n,X)
    Implicit None
    integer, Intent(in) :: n
    Real(PS), Intent(in), dimension(n) :: X
    Real(PS), dimension(n) :: Activation_identity
    Activation_identity = X
  End function Activation_identity

  function Activation_softmax(n,X)
    Implicit None
    Integer, Intent(in) :: n
    Real(PS), Intent(in), dimension(n) :: X
    Real(PS), dimension(n) :: tmp
    Real(PS), dimension(n) :: Activation_softmax
    tmp = exp(X - maxval(X))/sum(tmp)
    Activation_softmax = 1.0 / (1.0+exp(-X))
  End function Activation_softmax
  !↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑End Activation function↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑

  !↓↓↓↓↓↓↓↓↓↓ Generate_Training_Python file↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓
  !
  !  Very ugly subroutines. Simply write to .py file
  !  line by line. Will come back for further development
  !
  Subroutine Generate_Training_PY(file_num)
    Implicit None
    Integer :: file_num

    Call Generate_Training_M

  End Subroutine Generate_Training_PY

  Subroutine Generate_Training_main
    Implicit None
    Integer :: file_num


  End Subroutine Generate_Training_main

  Subroutine Generate_Training_PY_imports(file_num)
    Implicit None
    Integer :: file_num
    write(file_num,'(A)') "import json"
    write(file_num,'(A)') "import numpy as np"
    write(file_num,'(A)') "from sklearn.neural_network import MLPRegressor"
    write(file_num,'(A)') "from sklearn.ensemble import RandomForestRegressor"
    write(file_num,'(A)') "from sklearn import tree"
    write(file_num,'(A)') "from sklearn.multioutput import MultiOutputRegressor"
    write(file_num,'(A)') "import os"
    write(file_num,'(A)') ""

  End Subroutine Generate_Training_PY_imports

  Subroutine Generate_Training_PY_SK2F(file_num)
    Implicit None
    Integer :: file_num
    write(file_num,'(A)') "def sk2f(write_paras):"
    write(file_num,'(A)') "    if (write_paras.type == 'Neural_Network'):"
    write(file_num,'(A)') "        nn_sk2f(write_paras)"
    write(file_num,'(A)') "    elif (write_paras.type == 'Random_Forest'):"
    write(file_num,'(A)') "        rf_sk2f(write_paras)"
    write(file_num,'(A)') "    elif (write_paras.type == 'Decision_Tree'):"
    write(file_num,'(A)') "        dt_sk2f(write_paras)"
    write(file_num,'(A)') "    else:"
    write(file_num,'(A)') "        print('wrong type')"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "def nn_sk2f(nn):"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "    file_name = 'nn_param.dat'"
    write(file_num,'(A)') "    nn_output  = open(nn.output_param_path+file_name, 'w')"
    write(file_num,'(A)') "    nn_output.write('! n_layers\n%d\n'%(nn.n_layers_))"
    write(file_num,'(A)') "    nn_output.write('! layer_sizes\n')"
    write(file_num,'(A)') "    nn_output.write('%d \n'%np.shape(nn.coefs_[0])[0])"
    write(file_num,'(A)') "    for i in range(len(nn.hidden_layer_sizes)):"
    write(file_num,'(A)') "        nn_output.write('%d '%nn.hidden_layer_sizes[i])"
    write(file_num,'(A)') "    nn_output.write('%d\n'%nn.n_outputs_)"
    write(file_num,'(A)') "    nn_output.write('! intercepts\n')"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "    for i in range(len(nn.intercepts_)):"
    write(file_num,'(A)') "        for j in range(len(nn.intercepts_[i])):"
    write(file_num,'(A)') "            nn_output.write('%f '%nn.intercepts_[i][j])"
    write(file_num,'(A)') "        nn_output.write('\n')"
    write(file_num,'(A)') "    nn_output.write('! coefs\n')"
    write(file_num,'(A)') "    for i in range(len(nn.coefs_)):"
    write(file_num,'(A)') "        nn_output.write('!! layer%d\n'%i)"
    write(file_num,'(A)') "        coef = np.transpose(nn.coefs_[i])"
    write(file_num,'(A)') "        for j in range(len(coef)):"
    write(file_num,'(A)') "            for k in range(len(coef[j])):"
    write(file_num,'(A)') "                nn_output.write('%f '%coef[j][k])"
    write(file_num,'(A)') "            nn_output.write('\n')"
    write(file_num,'(A)') "    nn_output.write('! activations\n')"
    write(file_num,'(A)') "    nn_output.write(nn.activation)"
    write(file_num,'(A)') "    nn_output.write('\n')"
    write(file_num,'(A)') "    nn_output.write('! out_activations\n')"
    write(file_num,'(A)') "    nn_output.write(nn.out_activation_)"
    write(file_num,'(A)') "    nn_output.write('\n')"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "def dt_sk2f(dt):"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "    dt_output = open(dt.output_param_path+'dt_param.dat','w')"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "    dt_output.write('! node_count\n')"
    write(file_num,'(A)') "    dt_output.write('%d\n'%dt.tree_.node_count)"
    write(file_num,'(A)') "    dt_output.write('! n_features\n')"
    write(file_num,'(A)') "    dt_output.write('%d\n'%dt.tree_.n_features)"
    write(file_num,'(A)') "    dt_output.write('! n_outputs\n')"
    write(file_num,'(A)') "    dt_output.write('%d\n'%dt.tree_.n_outputs)"
    write(file_num,'(A)') "    dt_output.write('! max_depth\n')"
    write(file_num,'(A)') "    dt_output.write('%d\n'%dt.tree_.max_depth)"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "    for i in range(dt.tree_.node_count):"
    write(file_num,'(A)') "        dt_output.write('! node %d\n'%(i+1))"
    write(file_num,'(A)') "        dt_output.write('%d\n'%(dt.tree_.children_left[i]+1))"
    write(file_num,'(A)') "        dt_output.write('%d\n'%(dt.tree_.children_right[i]+1))"
    write(file_num,'(A)') "        dt_output.write('%d\n'%(dt.tree_.feature[i]+1))"
    write(file_num,'(A)') "        dt_output.write('%f\n'%dt.tree_.threshold[i])"
    write(file_num,'(A)') "        for j in range(dt.tree_.n_outputs):"
    write(file_num,'(A)') "            dt_output.write('%f '%dt.tree_.value[i,j])"
    write(file_num,'(A)') "        dt_output.write('\n')"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "def rf_sk2f(rf):"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "    rf_output = open(rf.output_param_path+'rf_param.dat','w')"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "    rf_output.write('! tree_count\n')"
    write(file_num,'(A)') "    rf_output.write('%d\n'%len(rf.estimators_))"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "    trees = rf.estimators_"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "    for j in range(len(rf.estimators_)):"
    write(file_num,'(A)') "        tree1 = trees[j].tree_"
    write(file_num,'(A)') "        rf_output.write('! node_count\n')"
    write(file_num,'(A)') "        rf_output.write('%d\n'%tree1.node_count)"
    write(file_num,'(A)') "        rf_output.write('! n_features\n')"
    write(file_num,'(A)') "        rf_output.write('%d\n'%tree1.n_features)"
    write(file_num,'(A)') "        rf_output.write('! n_outputs\n')"
    write(file_num,'(A)') "        rf_output.write('%d\n'%tree1.n_outputs)"
    write(file_num,'(A)') "        rf_output.write('! max_depth\n')"
    write(file_num,'(A)') "        rf_output.write('%d\n'%tree1.max_depth)"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "        for i in range(tree1.node_count):"
    write(file_num,'(A)') "            rf_output.write('! node %d\n'%(i+1))"
    write(file_num,'(A)') "            rf_output.write('%d\n'%(tree1.children_left[i]+1))"
    write(file_num,'(A)') "            rf_output.write('%d\n'%(tree1.children_right[i]+1))"
    write(file_num,'(A)') "            rf_output.write('%d\n'%(tree1.feature[i]+1))"
    write(file_num,'(A)') "            rf_output.write('%f\n'%tree1.threshold[i])"
    write(file_num,'(A)') "            for j in range(tree1.n_outputs):"
    write(file_num,'(A)') "                rf_output.write('%f '%tree1.value[i,j])"
    write(file_num,'(A)') "            rf_output.write('\n')"
    write(file_num,'(A)') ""
    write(file_num,'(A)') ""

  End Subroutine Generate_Training_PY_SK2F
  !↑↑↑↑↑↑↑↑↑↑↑↑↑End generating python file↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑
  
End Module Mod_Fsklearn_Essential
